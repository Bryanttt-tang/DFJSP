# ðŸ­ Realistic Dynamic Scheduling Design

## Date: November 1, 2025

---

## ðŸ“‹ **OVERVIEW**

We've implemented a **comprehensive realistic scheduling framework** that addresses critical issues with the original design:

### **Original Problems:**
1. âŒ Job arrival sequence was predetermined (J3 â†’ J4 â†’ J5 â†’ ...)
2. âŒ All machines had similar processing times (no heterogeneity)
3. âŒ No strategic reason to wait for specific jobs
4. âŒ Naive wait penalty (-5.0) didn't teach intelligent waiting
5. âŒ Proactive scheduling couldn't work without knowing which job arrives

### **New Solution:**
1. âœ… **Job categorization** (SHORT/MODERATE/LONG) with distinct processing profiles
2. âœ… **Machine heterogeneity** (FAST/MEDIUM/SLOW) creating strategic depth
3. âœ… **Uncertain arrival sequence** (which job arrives is stochastic)
4. âœ… **Soft probabilistic patterns** (realistic but not deterministic)
5. âœ… **Context-aware wait reward** (teaches when to wait vs when to schedule)

---

## ðŸ”§ **IMPLEMENTATION DETAILS**

### **1. Realistic Dataset Generation (`utils.py`)**

#### **A. Job Categorization**

```python
Job Categories:
---------------
- SHORT jobs:    1-2 operations, proc_time 5-15 per operation
- MODERATE jobs: 2-4 operations, proc_time 15-30 per operation  
- LONG jobs:     3-5 operations, proc_time 30-50 per operation

Distribution:
- 50% SHORT
- 30% MODERATE
- 20% LONG
```

**Example Processing Times:**
```
SHORT job (J0):
  Op0: M0:8, M1:8, M2:8, M3:7 (fast!)       â†’ Best: 7 time units
  Op1: M0:5, M1:4, M2:4, M3:3 (fast!)       â†’ Best: 3 time units
  Total: ~10 time units

LONG job (J12):
  Op0: M0:37, M1:37, M2:36, M3:31, M5:42    â†’ Best: 31, Worst: 42
  Op1: M0:36, M1:34, M2:41, M3:33, M5:47    â†’ Best: 33, Worst: 47
  Op2: M0:47, M1:44, M2:45, M3:38, M5:51    â†’ Best: 38, Worst: 51
  Op3: M0:40, M1:41, M3:32, M4:47           â†’ Best: 32, Worst: 47
  Total: ~140-190 time units
```

#### **B. Machine Heterogeneity**

```python
Machine Categories:
-------------------
- FAST machines (25%):    speed_factor = 0.6-0.8  (20-40% faster)
- MEDIUM machines (50%):  speed_factor = 0.9-1.1  (baseline)
- SLOW machines (25%):    speed_factor = 1.2-1.5  (20-50% slower)

Processing Time Calculation:
actual_time = base_time * machine_speed_factor * noise(0.9-1.1)
```

**Example from Generated Data:**
```
Machine M3: FAST   (speed_factor: 0.82) 
Machine M0: MEDIUM (speed_factor: 1.00)
Machine M5: SLOW   (speed_factor: 1.19)

Long job operation on different machines:
- M3 (fast):   30 * 0.82 = 25 time units
- M0 (medium): 30 * 1.00 = 30 time units
- M5 (slow):   30 * 1.19 = 36 time units

Difference: 36 - 25 = 11 time units (44% difference!)
```

**Strategic Implication:**
```
Scenario: Machine M3 (fast) is busy until t=20, M5 (slow) is free now
Question: Schedule long job on M5 NOW, or WAIT for M3?

Option A: Schedule on M5 NOW
  â†’ Start: t=current (e.g., t=10)
  â†’ Duration: 36 time units
  â†’ Finish: t=46

Option B: WAIT for M3 (free at t=20)
  â†’ Start: t=20
  â†’ Duration: 25 time units
  â†’ Finish: t=45

WAITING WINS by 1 time unit!
```

---

### **2. Realistic Arrival Sequence**

#### **A. Uncertain Job Sequence**

**OLD (WRONG):**
```python
# Predetermined sequence
J0, J1, J2 â†’ (initial jobs at t=0)
J3 arrives at t=8
J4 arrives at t=12
J5 arrives at t=16
...

Problem: Agent knows J3, J4, J5 will arrive in this order!
```

**NEW (CORRECT):**
```python
# Uncertain sequence - only TIMING known, not WHICH job
t=0:  J0, J1, J2, J3, J4 (initial jobs)
t=6:  ??? (could be J8, J12, J5... randomly selected from future jobs)
t=14: ??? (another random selection)
t=22: ??? (another random selection)
...

Agent only knows:
- An arrival will happen at tâ‰ˆ8 (Poisson sampling)
- It will likely be SHORT/MODERATE/LONG (pattern hints)
- But WHICH SPECIFIC JOB? Unknown!
```

#### **B. Soft Probabilistic Patterns**

**Pattern Rules (pattern_strength=0.5):**

```python
Base Distribution:
- 50% SHORT
- 30% MODERATE  
- 20% LONG

Pattern Adjustments:
1. After 4+ SHORT jobs â†’ Increase LONG probability to 50%
   (Workload balancing - expecting heavier jobs)

2. After 1+ LONG job â†’ Increase SHORT probability to 70%
   (Recovery period - lighter jobs follow heavy ones)

3. After MODERATE job â†’ Balanced distribution
   (Transitional state)

Final Probability = (1 - strength) * base + strength * pattern
                  = 0.5 * base + 0.5 * pattern
```

**Example Arrival Sequence:**
```
t=0:    J0 (short), J1 (long), J2 (moderate), J3 (moderate), J4 (long) [initial]
t=5.9:  J8 (long)      â† Random selection, but pattern: after LONG, expect SHORT
t=22.3: J5 (short)     â† Pattern working! (SHORT after LONG cluster)
t=24.4: J16 (short)    â† SHORT after SHORT (base distribution)
t=25.2: J12 (long)     â† After 2 SHORT, increasing LONG probability
t=36.7: J7 (moderate)  â† Transition
t=36.9: J6 (moderate)  â† Moderate cluster
t=59.3: J18 (short)    â† Back to SHORT
t=61.8: J10 (short)    â† SHORT cluster forming...
t=66.3: J19 (short)    
t=73.4: J13 (short)    
t=85.2: J17 (short)    
t=89.5: J15 (short)    â† 5 SHORT in a row!
t=97.1: J9 (moderate)  â† Pattern: After 5 SHORT, expect LONG/MODERATE
t=99.9: J14 (short)
t=111:  J11 (moderate)
```

**Pattern Analysis:**
```
Transitions observed:
  After SHORT:    60% SHORT, 20% MODERATE, 20% LONG
  After MODERATE: 40% SHORT, 40% MODERATE, 20% LONG
  After LONG:     25% SHORT, 50% MODERATE, 25% LONG

Not perfectly deterministic, but shows learnable structure!
```

---

### **3. Context-Aware Wait Reward**

**Function: `calculate_context_aware_wait_reward(env)`**

#### **Decision Logic:**

```python
Case 1: Idle Machines + Available Work = BAD WAIT!
-------------------------------------------------------
Condition: num_idle_machines > 0 AND num_schedulable_ops > 0

Penalty = - (idle_machines * wait_duration * 0.5) * work_multiplier

where work_multiplier = 1 + (total_available_proc_time / 100)

Example:
- 3 idle machines
- Wait duration: 10 time units
- Available work: 50 time units of operations
- Penalty = -(3 * 10 * 0.5) * (1 + 50/100) = -15 * 1.5 = -22.5

Agent learns: "Don't wait when you have work and idle machines!"


Case 2: Job Arriving Soon = ACCEPTABLE WAIT
--------------------------------------------
Condition: Next event is arrival AND wait_duration < 3.0

Reward = -0.1 * wait_duration

Example:
- Next arrival at t+2
- Wait duration: 2 time units
- Penalty = -0.1 * 2 = -0.2

Agent learns: "Waiting is OK when job arriving very soon"


Case 3: No Work Available = NEUTRAL WAIT
-----------------------------------------
Condition: num_schedulable_ops == 0

Reward = -0.1 * wait_duration

Example:
- All arrived jobs scheduled
- Wait duration: 5 time units
- Penalty = -0.1 * 5 = -0.5

Agent learns: "No choice but to wait when no work available"


Case 4: All Machines Busy = OK WAIT
------------------------------------
Condition: num_idle_machines == 0

Reward = -0.05 * wait_duration

Example:
- All machines occupied
- Wait duration: 8 time units
- Penalty = -0.05 * 8 = -0.4

Agent learns: "Forced to wait when machines busy"


Case 5: Default = MODERATE PENALTY
-----------------------------------
Reward = -1.0 * wait_duration

For edge cases not covered above.
```

#### **Why This Design Works:**

```
Traditional Naive Penalty:
wait_reward = -5.0 if scheduling_possible else -1.0

Problems:
- Same penalty whether waiting 1 or 10 time units
- No distinction between "waiting with 1 idle machine" vs "3 idle machines"
- Doesn't reward strategic waiting (e.g., waiting for fast machine)

Context-Aware Penalty:
Considers:
âœ“ Number of idle machines (opportunity cost)
âœ“ Amount of available work (urgency)
âœ“ Wait duration (time wastage)
âœ“ Arrival timing (strategic waiting)

Result:
âœ“ Agent learns WHEN to wait strategically
âœ“ Understands tradeoff: "schedule on slow machine NOW vs wait for fast machine"
âœ“ Differentiates forced waiting from bad waiting
```

---

## ðŸ“Š **USAGE**

### **Generate Realistic Dataset:**

```python
from utils import generate_realistic_fjsp_dataset, generate_realistic_arrival_sequence

# Generate jobs with categories and machine heterogeneity
jobs_data, machine_list, machine_metadata = generate_realistic_fjsp_dataset(
    num_initial_jobs=5,
    num_future_jobs=15,
    total_num_machines=6,
    job_type_distribution={'short': 0.5, 'moderate': 0.3, 'long': 0.2},
    machine_speed_variance=0.5,  # 0=no variance, 1=high variance
    seed=42
)

# Generate arrival sequence with patterns
arrival_times, arrival_sequence = generate_realistic_arrival_sequence(
    jobs_data=jobs_data,
    num_initial_jobs=5,
    arrival_rate=0.08,
    pattern_strength=0.5,  # 0=random, 1=deterministic, 0.5=realistic
    seed=42
)

# Print information
from utils import print_dataset_info, print_dataset_table
print_dataset_table(jobs_data, machine_list, machine_metadata)
```

### **Train Reactive RL Agent:**

```python
# Create environment with realistic data
env = PoissonDynamicFJSPEnv(
    jobs_data=ENHANCED_JOBS_DATA,  # Normalized format (just operations)
    machine_list=MACHINE_LIST,
    initial_jobs=5,
    arrival_rate=0.08,
    max_time_horizon=200,
    reward_mode="makespan_increment",
    seed=12345
)

# Environment automatically uses:
# - Realistic arrival sequence from DETERMINISTIC_ARRIVAL_TIMES
# - Context-aware wait reward function
# - Uncertain job arrival order

# Train with MaskablePPO
from sb3_contrib import MaskablePPO
model = MaskablePPO(
    "MlpPolicy",
    env,
    ent_coef=0.0,  # CRITICAL: No entropy bonus for deterministic problem!
    learning_rate=1e-5,  # Lower LR for stable convergence
    n_steps=2048,
    batch_size=256,
    gamma=1.0,  # Correct for makespan_increment
    verbose=1
)

model.learn(total_timesteps=500000)  # Train longer!
```

---

## ðŸŽ¯ **EXPECTED LEARNING OUTCOMES**

### **What the Agent Should Learn:**

#### **1. Machine Selection Strategy:**
```
SHORT job available, machines:
- M3 (fast, busy until t=10): proc_time = 7
- M5 (slow, free now):        proc_time = 12

Agent learns:
â†’ IF current_time < 5: WAIT for M3 (saves 5 time units)
â†’ IF current_time > 5: Schedule on M5 NOW (waiting too costly)
```

#### **2. Job Prioritization Based on Patterns:**
```
Observations:
- 4 SHORT jobs arrived recently
- Pattern suggests LONG job likely next

Agent learns:
â†’ Schedule SHORT jobs aggressively on any machine
â†’ Reserve FAST machines for likely incoming LONG job
â†’ Strategic waiting for fast machines
```

#### **3. Waiting vs Scheduling Tradeoffs:**
```
Situation: 1 MODERATE job ready, 2 machines busy, 1 fast machine free

Context-aware penalty teaches:
â†’ Schedule immediately (idle machine + available work = bad wait)

Situation: All jobs scheduled, next arrival in 2 time units

Context-aware penalty teaches:
â†’ Wait is OK (no work available, arrival soon)
```

---

## ðŸ“ˆ **TRAINING IMPROVEMENTS**

### **Recommended Hyperparameters:**

```python
# CRITICAL FIXES for Convergence:

1. Entropy Coefficient:
   ent_coef=0.0  or  ent_coef=-0.01
   
   WHY: Deterministic scheduling problem, no need for exploration
        Old value (0.01) was FIGHTING convergence!

2. Learning Rate:
   learning_rate=1e-5  (was 5e-4)
   
   WHY: Smaller LR = smoother convergence, less overshooting

3. Training Duration:
   total_timesteps=500000  (was 100000)
   
   WHY: More complex problem (machine heterogeneity + patterns)
        Needs more experience to learn strategies

4. Network Architecture:
   net_arch=dict(pi=[256, 256], vf=[256, 128])  (was pi=[512,512,256,128])
   
   WHY: Simpler network converges faster for deterministic problems
```

### **Expected Performance:**

```
Baseline (Heuristics):
- FIFO: ~85-95 makespan
- SPT:  ~75-85 makespan (best heuristic)
- LPT:  ~90-100 makespan

Target (Reactive RL after fixes):
- Episode 0-1000:   ~100-120 makespan (exploration)
- Episode 1000-5000: ~80-100 makespan (learning)
- Episode 5000+:     ~70-80 makespan (converged)
- GOAL: Match or beat SPT heuristic (~75)

Perfect Knowledge RL (oracle arrivals):
- With fixes: ~60-70 makespan
- GOAL: Approach MILP optimal (~55-60)
```

---

## ðŸ”¬ **DIAGNOSTIC TOOLS**

### **Check if Patterns are Working:**

```python
# After generating arrivals
from collections import Counter

# Analyze transitions
transitions = []
for i in range(len(ARRIVAL_SEQUENCE) - 1):
    current_type = ARRIVAL_SEQUENCE[i][2][0].upper()
    next_type = ARRIVAL_SEQUENCE[i+1][2][0].upper()
    transitions.append(f"{current_type}â†’{next_type}")

print(Counter(transitions))

# Expected (with pattern_strength=0.5):
# Sâ†’S: ~40-50%  (short jobs cluster)
# Sâ†’L: ~15-25%  (after cluster, long arrives)
# Lâ†’S: ~50-70%  (after long, short jobs)
# Mâ†’M: ~30-40%  (moderate transitions)
```

### **Check Machine Heterogeneity Impact:**

```python
# Analyze processing time variance
for job_id, operations in ENHANCED_JOBS_DATA.items():
    for op_idx, operation in enumerate(operations):
        proc_times = operation['proc_times']
        if len(proc_times) > 1:
            min_time = min(proc_times.values())
            max_time = max(proc_times.values())
            variance = (max_time - min_time) / min_time * 100
            
            if variance > 30:  # More than 30% difference
                print(f"J{job_id}-O{op_idx}: Variance {variance:.0f}%")
                print(f"  Fast: {min_time}, Slow: {max_time}")

# Expected: 20-50% variance for most operations
# This creates strategic waiting decisions!
```

### **Monitor Wait Reward Distribution:**

```python
# During training, log wait rewards
wait_rewards = []

# In step() function, add:
if job_idx is None:  # WAIT action
    wait_rewards.append(wait_reward)

# After episode:
print(f"Wait rewards: mean={np.mean(wait_rewards):.2f}, "
      f"std={np.std(wait_rewards):.2f}, "
      f"min={np.min(wait_rewards):.2f}, "
      f"max={np.max(wait_rewards):.2f}")

# Expected:
# - Mean: -2.0 to -5.0 (context-dependent)
# - Std:  2.0 to 5.0 (high variance = learning opportunities!)
# - Min:  -20.0 to -30.0 (bad waits heavily penalized)
# - Max:  -0.1 to -0.5 (good waits lightly penalized)
```

---

## âœ… **SUMMARY OF CHANGES**

### **Files Modified:**

#### **1. `utils.py`**
- âœ… Added `generate_realistic_fjsp_dataset()` - Job categorization + machine heterogeneity
- âœ… Added `generate_realistic_arrival_sequence()` - Uncertain sequence + soft patterns
- âœ… Added `_sample_next_job_type()` - Pattern-based job type sampling
- âœ… Added `_create_machine_categories()` - Machine speed factor generation
- âœ… Updated `print_dataset_info()` - Handle metadata format
- âœ… Updated `print_dataset_table()` - Show machine categories and job types

#### **2. `proactive_sche.py`**
- âœ… Added `calculate_context_aware_wait_reward()` - Smart wait penalty
- âœ… Added `normalize_jobs_data()` - Convert metadata format to simple format
- âœ… Updated dataset generation - Use realistic functions
- âœ… Updated PoissonDynamicFJSPEnv - Use smart wait reward
- âœ… Generated arrival sequence stored in `DETERMINISTIC_ARRIVAL_TIMES`

---

## ðŸš€ **NEXT STEPS**

### **1. Test the New Design:**
```bash
python proactive_sche.py
# Check if:
# - Dataset generates correctly
# - Machine heterogeneity visible
# - Arrival patterns make sense
# - Training starts without errors
```

### **2. Train Reactive RL:**
```python
# Use recommended hyperparameters
# Monitor:
# - Entropy (should decrease to < -10)
# - Episode rewards (should improve steadily)
# - Wait action frequency (should become strategic, not random)
```

### **3. Analyze Results:**
```python
# Compare:
# - Reactive RL vs SPT heuristic
# - Wait reward distribution
# - Machine utilization (fast vs slow machines)
# - Job type scheduling patterns
```

### **4. Optional Enhancements:**
- Add imitation learning (pre-train on SPT heuristic)
- Implement hybrid SPT-RL (hard-code machine selection = SPT, learn job sequencing)
- Add observation features: recent arrival types, machine speed indicators
- Experiment with pattern_strength (0.3, 0.5, 0.7)

---

## ðŸŽ“ **KEY INSIGHTS**

1. **Machine heterogeneity is ESSENTIAL** for strategic waiting
   - Without it, no reason to wait for specific machines
   - Creates meaningful tradeoffs: schedule NOW vs WAIT

2. **Uncertain job sequence is REALISTIC**
   - Predetermined sequence = unrealistic oracle knowledge
   - Stochastic sequence = real industrial scenarios

3. **Soft patterns are LEARNABLE**
   - Pure random = too hard to learn
   - Deterministic patterns = agent overfits
   - Soft patterns (50% pattern, 50% random) = sweet spot!

4. **Context-aware rewards are CRUCIAL**
   - Naive penalties don't teach strategy
   - Context-aware penalties provide rich learning signals
   - Agent learns WHEN to wait, not just "waiting is bad"

5. **Entropy coefficient MUST be 0 or negative**
   - Scheduling is deterministic optimization
   - Entropy bonus fights convergence
   - This was the root cause of high entropy (-1.5) problem!

**The path to success: Realistic data + Smart rewards + Proper hyperparameters = Learning!** ðŸš€
