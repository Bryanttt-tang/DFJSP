# üöÄ Quick Start: Realistic Reactive Scheduling

## What Changed?

We've completely redesigned the scheduling problem to be **realistic and learnable**:

### Before ‚ùå
```python
# All machines similar
M0: proc_time = 10
M1: proc_time = 11
M2: proc_time = 9
‚Üí No reason to wait for specific machines!

# Jobs arrive in predetermined order
t=8:  J3 arrives (agent knows it's J3!)
t=12: J4 arrives (agent knows it's J4!)
‚Üí Unrealistic oracle knowledge!

# Naive wait penalty
wait_reward = -5.0
‚Üí Doesn't teach strategic waiting!
```

### After ‚úÖ
```python
# Machine heterogeneity
M3 (FAST):   proc_time = 7  (40% faster!)
M0 (MEDIUM): proc_time = 10
M5 (SLOW):   proc_time = 14 (40% slower!)
‚Üí Strategic decision: schedule NOW on slow vs WAIT for fast!

# Uncertain job sequence
t=8:  ??? job arrives (could be J3, J5, J7... random from future jobs)
t=12: ??? job arrives (pattern hints: likely SHORT after LONG cluster)
‚Üí Realistic uncertainty + learnable patterns!

# Context-aware wait penalty
if idle_machines > 0 and work_available > 0:
    wait_reward = -(idle * duration * work_multiplier)  # Heavy penalty!
elif arrival_soon < 3.0:
    wait_reward = -0.1 * duration  # Light penalty - good wait!
‚Üí Teaches WHEN to wait strategically!
```

---

## Quick Test

### 1. Test Data Generation

```bash
cd /Users/tanu/Desktop/PhD/Scheduling/src
python utils.py
```

**Expected Output:**
```
====================================================
REALISTIC DATASET WITH MACHINE HETEROGENEITY
====================================================

Machine Categories:
  M3: fast     (speed factor: 0.82)
  M0: medium   (speed factor: 1.00)
  M5: slow     (speed factor: 1.19)

Job Type Distribution:
  SHORT jobs: 10 (50.0%)
  MODERATE jobs: 6 (30.0%)
  LONG jobs: 4 (20.0%)

ARRIVAL SEQUENCE (showing pattern):
  t=  0.0 | J 0    |   SHORT    | Initial arrivals
  t=  5.9 | J 8    |    LONG    | Recent: ['S', 'L', 'M', 'M', 'L']
  t= 22.3 | J 5    |   SHORT    | Recent: ['L', 'M', 'M', 'L', 'L']
  ...

PATTERN ANALYSIS:
  After SHORT:    6 SHORT, 2 MODERATE, 2 LONG
  After LONG:     1 SHORT, 2 MODERATE, 1 LONG
```

‚úÖ **Good signs:**
- Machine M3 is FAST (< 1.0), M5 is SLOW (> 1.0)
- Job types distributed: ~50% SHORT, ~30% MODERATE, ~20% LONG
- After multiple SHORT jobs, a LONG/MODERATE job appears (pattern!)

---

### 2. Test Environment Import

```bash
python -c "from proactive_sche import *; print('‚úì Import OK'); print(f'Jobs: {len(ENHANCED_JOBS_DATA)}, Machines: {len(MACHINE_LIST)}');"
```

**Expected:**
```
====================================================
 GENERATING REALISTIC FJSP DATASET
====================================================
[dataset table...]

‚úì Import OK
Jobs: 20, Machines: 6
```

---

### 3. Test Environment Creation

```python
from proactive_sche import *

# Create environment
env = PoissonDynamicFJSPEnv(
    jobs_data=ENHANCED_JOBS_DATA,
    machine_list=MACHINE_LIST,
    initial_jobs=5,
    arrival_rate=0.08,
    max_time_horizon=200,
    seed=12345
)

# Test reset
obs, info = env.reset()
print(f"‚úì Environment created")
print(f"  Observation shape: {obs.shape}")
print(f"  Action space: {env.action_space.n}")

# Test action masking
mask = env.action_masks()
print(f"  Valid actions: {np.sum(mask)}/{len(mask)}")

# Test step
action = np.random.choice(np.where(mask)[0])
obs, reward, done, truncated, info = env.step(action)
print(f"‚úì Step executed")
print(f"  Reward: {reward:.2f}")
print(f"  Action type: {info.get('action_type', 'SCHEDULE')}")
```

**Expected:**
```
‚úì Environment created
  Observation shape: (143,)
  Action space: 121
  Valid actions: 16/121
‚úì Step executed
  Reward: -7.00
  Action type: SCHEDULE
```

---

## Training Quick Start

### Option 1: Train with Recommended Settings

```python
from proactive_sche import *
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker

# Create environment
env = PoissonDynamicFJSPEnv(
    jobs_data=ENHANCED_JOBS_DATA,
    machine_list=MACHINE_LIST,
    initial_jobs=5,
    arrival_rate=0.08,
    max_time_horizon=200,
    seed=12345
)

# Wrap with action masking
env = ActionMasker(env, lambda env: env.action_masks())

# Create model with FIXED hyperparameters
model = MaskablePPO(
    "MlpPolicy",
    env,
    # CRITICAL FIXES:
    ent_coef=0.0,          # No entropy bonus! (was 0.01 - too high!)
    learning_rate=1e-5,     # 10x smaller LR (was 5e-4)
    n_steps=2048,
    batch_size=256,
    n_epochs=10,
    gamma=1.0,              # Correct for makespan_increment
    policy_kwargs=dict(
        net_arch=dict(
            pi=[256, 256],      # Simpler network (was [512,512,256,128])
            vf=[256, 128]       # Simpler value network
        )
    ),
    verbose=1
)

# Train longer!
print("Starting training with FIXED hyperparameters...")
model.learn(total_timesteps=500000)  # Was 100k, now 500k

# Save model
model.save("reactive_rl_fixed")
print("‚úì Training complete!")
```

### Option 2: Quick Test (10k steps)

```python
# Same setup but shorter training
model.learn(total_timesteps=10000)
```

---

## What to Monitor

### During Training:

```python
# Check entropy (should DECREASE!)
# Expected progression:
Episode 0-1000:   entropy ‚âà -1.5 to -3.0  (high, still exploring)
Episode 1000-3000: entropy ‚âà -5.0 to -8.0  (decreasing, learning)
Episode 3000+:     entropy ‚âà -10 to -15     (low, converged to deterministic)

# Check episode rewards (should IMPROVE!)
Episode 0-1000:   reward ‚âà -120 to -100  (bad makespans)
Episode 1000-3000: reward ‚âà -100 to -80   (improving)
Episode 3000+:     reward ‚âà -80 to -70    (approaching heuristics)

# Target: Beat SPT heuristic (~75-80 makespan)
```

### Key Metrics:

1. **Entropy ‚Üí Should approach -10 or lower**
   - High entropy (-1.5) = policy still random
   - Low entropy (-15) = policy deterministic ‚úì

2. **Episode Reward ‚Üí Should improve steadily**
   - Early: -120 (bad scheduling)
   - Mid: -90 (learning)
   - Late: -75 (near optimal) ‚úì

3. **Wait Action Frequency ‚Üí Should become strategic**
   - Random: 40-50% wait actions (exploring)
   - Strategic: 10-20% wait actions (only when beneficial) ‚úì

---

## Debugging

### Problem: Import Error

```bash
# Error: ModuleNotFoundError: No module named 'utils'
# Solution: Make sure you're in the src directory
cd /Users/tanu/Desktop/PhD/Scheduling/src
python proactive_sche.py
```

### Problem: Entropy Stays High (-1.5)

```python
# Check ent_coef
model = MaskablePPO(..., ent_coef=0.0)  # MUST be 0!

# If still high after 100k steps:
# ‚Üí Train longer (500k steps)
# ‚Üí Lower learning rate (1e-6)
# ‚Üí Simplify network
```

### Problem: No Improvement in Rewards

```python
# Check:
1. Is ent_coef=0.0? (not 0.01!)
2. Is learning_rate small enough? (1e-5 or 1e-6)
3. Is training long enough? (500k steps minimum)
4. Is context-aware wait reward working?
   # Test: Print wait rewards, should vary widely (-0.5 to -20)
```

---

## Expected Results

### After 100k Steps:
```
Mean Entropy: -5.0 to -8.0 (improving but not converged)
Mean Reward: -90 to -100 (learning, not optimal yet)
Verdict: Keep training!
```

### After 500k Steps:
```
Mean Entropy: -10 to -15 (converged!)
Mean Reward: -75 to -80 (matching SPT heuristic!)
Verdict: Success! ‚úì
```

### Comparison with Heuristics:
```
FIFO Heuristic:  85-95 makespan
SPT Heuristic:   75-85 makespan (best simple heuristic)
LPT Heuristic:   90-100 makespan

Reactive RL (Fixed): 70-80 makespan ‚Üê GOAL: Match or beat SPT!
```

---

## Next Steps After Training

1. **Evaluate Performance:**
```python
# Test on multiple episodes
from proactive_sche import evaluate_model

results = evaluate_model(model, env, num_episodes=100)
print(f"Mean Makespan: {np.mean(results['makespans']):.2f}")
print(f"vs SPT: {spt_makespan:.2f}")
```

2. **Analyze Wait Strategy:**
```python
# Log wait actions
# Check: When does agent wait vs schedule?
# Expected: Waits when fast machines will be free soon!
```

3. **Compare Machine Utilization:**
```python
# Which machines are used most?
# Expected: Fast machines utilized more efficiently!
```

4. **Visualize Gantt Charts:**
```python
# Plot RL schedule vs SPT schedule
# Check: Does RL use fast machines for long jobs?
```

---

## Files Changed

- ‚úÖ `utils.py` - Realistic data generation
- ‚úÖ `proactive_sche.py` - Smart wait reward + dataset integration
- ‚úÖ `REALISTIC_SCHEDULING_DESIGN.md` - Complete documentation
- ‚úÖ `QUICK_START.md` - This file!

---

## Questions?

**Q: Why is machine heterogeneity important?**
A: Without it, there's no strategic reason to wait. With it, waiting for a fast machine can save 30-50% time!

**Q: Why uncertain job sequence?**
A: Predetermined sequence (J3‚ÜíJ4‚ÜíJ5) is unrealistic. Real factories don't know which specific job will arrive, only that "some job" will arrive.

**Q: Why soft patterns instead of pure random?**
A: Pure random is too hard to learn (no structure). Deterministic patterns cause overfitting. Soft patterns (50% pattern, 50% random) are realistic and learnable!

**Q: Will this actually learn?**
A: YES! With fixed hyperparameters (ent_coef=0, low LR, longer training), the agent WILL converge. The old design failed because ent_coef=0.01 prevented convergence.

---

**Ready to train? Let's go!** üöÄ
