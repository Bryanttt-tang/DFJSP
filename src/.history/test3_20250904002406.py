import gym
import numpy as np
import matplotlib.pyplot as plt
from gym import spaces
import random
from stable_baselines3.common.vec_env import DummyVecEnv
from sb3_contrib import MaskablePPO
from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy
from sb3_contrib.common.maskable.utils import get_action_masks
from sb3_contrib.common.wrappers import ActionMasker
from stable_baselines3.common.monitor import Monitor
import os
import collections
from pulp import LpProblem, LpMinimize, LpVariable, lpSum, PULP_CBC_CMD
import argparse
import importlib.util
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)

# --- Default Instance Data ---
DEFAULT_JOBS_DATA = collections.OrderedDict({
    0: [{'proc_times': {'M0': 5, 'M1': 7}}, {'proc_times': {'M1': 6, 'M2': 4}}, {'proc_times': {'M0': 3}}],
    1: [{'proc_times': {'M1': 8, 'M2': 6}}, {'proc_times': {'M0': 5}}, {'proc_times': {'M1': 4, 'M2': 5}}],
    2: [{'proc_times': {'M0': 6, 'M2': 7}}, {'proc_times': {'M0': 4, 'M1': 5}}, {'proc_times': {'M2': 8}}],
    3: [{'proc_times': {'M1': 9}}, {'proc_times': {'M2': 3}}, {'proc_times': {'M0': 6, 'M1': 7}}]
})
DEFAULT_MACHINE_LIST = ['M0', 'M1', 'M2']
DEFAULT_ARRIVAL_TIMES = {0: 0, 1: 0, 2: 10, 3: 15}

# --- Extended 7-Job Instance Data ---
EXTENDED_JOBS_DATA = collections.OrderedDict({
    0: [{'proc_times': {'M0': 4, 'M1': 6}}, {'proc_times': {'M1': 5, 'M2': 3}}, {'proc_times': {'M0': 2}}],
    1: [{'proc_times': {'M1': 7, 'M2': 5}}, {'proc_times': {'M0': 4}}, {'proc_times': {'M1': 3, 'M2': 4}}],
    2: [{'proc_times': {'M0': 5, 'M2': 6}}, {'proc_times': {'M0': 3, 'M1': 4}}, {'proc_times': {'M2': 7}}],
    3: [{'proc_times': {'M1': 8}}, {'proc_times': {'M2': 2}}, {'proc_times': {'M0': 5, 'M1': 6}}],
    4: [{'proc_times': {'M0': 6, 'M1': 9}}, {'proc_times': {'M1': 7, 'M2': 5}}, {'proc_times': {'M0': 4}}, {'proc_times': {'M2': 6}}],
    5: [{'proc_times': {'M1': 5, 'M2': 8}}, {'proc_times': {'M0': 6}}, {'proc_times': {'M1': 4, 'M2': 3}}],
    6: [{'proc_times': {'M0': 7, 'M2': 4}}, {'proc_times': {'M0': 5, 'M1': 6}}, {'proc_times': {'M1': 3}}, {'proc_times': {'M0': 2, 'M2': 5}}]
})
EXTENDED_MACHINE_LIST = ['M0', 'M1', 'M2']
EXTENDED_ARRIVAL_TIMES = {0: 0, 1: 0, 2: 0, 3: 10, 4: 15, 5: 25, 6: 35}

# ...existing code...

def load_instance_from_file(filepath):
    """Load instance data from a Python file generated by possison.py"""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Instance file not found: {filepath}")
    
    try:
        # Method 1: Direct execution with exec()
        with open(filepath, 'r') as f:
            content = f.read()
        
        # Create a namespace to execute the file content
        namespace = {}
        exec(content, namespace)
        
        # Extract the required variables
        jobs_data = namespace.get('jobs_data', None)
        machine_list = namespace.get('machine_list', None)
        job_arrival_times = namespace.get('job_arrival_times', None)
        
        if jobs_data is None or machine_list is None or job_arrival_times is None:
            raise ValueError(f"Instance file {filepath} must contain 'jobs_data', 'machine_list', and 'job_arrival_times'")
        
        print(f"Loaded instance from {filepath}")
        print(f"Jobs: {len(jobs_data)}, Machines: {len(machine_list)}")
        print(f"Arrival times: {job_arrival_times}")
        
        return jobs_data, machine_list, job_arrival_times
        
    except Exception as e:
        print(f"Error reading file with exec method: {e}")
        
        # Method 2: Fallback to importlib (with better error handling)
        try:
            # Get absolute path
            abs_filepath = os.path.abspath(filepath)
            
            # Load the module dynamically
            spec = importlib.util.spec_from_file_location("instance_module", abs_filepath)
            
            if spec is None or spec.loader is None:
                raise ImportError(f"Could not create module spec for {abs_filepath}")
            
            instance_module = importlib.util.module_from_spec(spec)
            
            # Add to sys.modules to avoid import issues
            sys.modules["instance_module"] = instance_module
            
            spec.loader.exec_module(instance_module)
            
            # Extract the required variables
            jobs_data = getattr(instance_module, 'jobs_data', None)
            machine_list = getattr(instance_module, 'machine_list', None)
            job_arrival_times = getattr(instance_module, 'job_arrival_times', None)
            
            if jobs_data is None or machine_list is None or job_arrival_times is None:
                raise ValueError(f"Instance file {filepath} must contain 'jobs_data', 'machine_list', and 'job_arrival_times'")
            
            print(f"Loaded instance from {filepath}")
            print(f"Jobs: {len(jobs_data)}, Machines: {len(machine_list)}")
            print(f"Arrival times: {job_arrival_times}")
            
            return jobs_data, machine_list, job_arrival_times
            
        except Exception as e2:
            raise RuntimeError(f"Failed to load instance file {filepath}. Exec error: {e}, Import error: {e2}")

# --- 1. Gantt Chart Plotter ---
def plot_gantt(schedule, machines, title="Schedule"):
    """Plot Gantt chart for the schedule"""
    if not schedule or all(len(ops) == 0 for ops in schedule.values()):
        print("No schedule to plot - schedule is empty")
        return

    colors = plt.cm.tab20.colors
    fig, ax = plt.subplots(figsize=(12, len(machines) * 0.8))

    for idx, m in enumerate(machines):
        machine_ops = schedule.get(m, [])
        machine_ops.sort(key=lambda x: x[1])

        for op_data in machine_ops:
            if len(op_data) == 3:
                job_id_str, start, end = op_data
                try:
                    # Extract job number from "J0-O1"
                    j = int(job_id_str.split('-')[0][1:])
                except (ValueError, IndexError):
                    j = hash(job_id_str) % len(colors)
                
                ax.broken_barh(
                    [(start, end - start)],
                    (idx * 10, 8),
                    facecolors=colors[j % len(colors)],
                    edgecolor='black',
                    alpha=0.8
                )
                label = job_id_str
                ax.text(start + (end - start) / 2, idx * 10 + 4,
                       label, color='white', fontsize=10,
                       ha='center', va='center', weight='bold')

    ax.set_yticks([i * 10 + 4 for i in range(len(machines))])
    ax.set_yticklabels(machines)
    ax.set_xlabel("Time")
    ax.set_ylabel("Machines")
    ax.set_title(title)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# --- 2. Dynamic RL Environment ---
def mask_fn(env: gym.Env) -> np.ndarray:
    return env.action_masks()

class GraphDFJSPEnv(gym.Env):
    """
    Graph-based DFJSP environment using node features and adjacency matrices.
    More scalable and expressive than fixed-size observation spaces.
    """
    
    def __init__(self, jobs_data, machine_list, job_arrival_times=None):
        super().__init__()
        self.jobs = jobs_data
        self.machines = machine_list
        self.job_ids = list(self.jobs.keys())
        self.num_jobs = len(self.job_ids)
        self.num_machines = len(self.machines)
        self.job_arrival_times = job_arrival_times or {j: 0 for j in self.job_ids}
        
        # Create operation list and mappings
        self.operations = []  # [(job_id, op_idx), ...]
        self.job_to_ops = {}  # job_id -> [op_indices]
        
        op_idx = 0
        for job_id in self.job_ids:
            self.job_to_ops[job_id] = []
            for op_pos in range(len(self.jobs[job_id])):
                self.operations.append((job_id, op_pos))
                self.job_to_ops[job_id].append(op_idx)
                op_idx += 1
        
        self.num_operations = len(self.operations)
        
        # Action space: select (operation, machine) pair
        valid_pairs = []
        for op_idx, (job_id, op_pos) in enumerate(self.operations):
            for machine in self.jobs[job_id][op_pos]['proc_times']:
                valid_pairs.append((op_idx, machine))
        
        self.valid_action_pairs = valid_pairs
        self.action_space = spaces.Discrete(len(valid_pairs))
        
        # Graph-based observation space with enhanced features
        self.observation_space = spaces.Dict({
            'node_features': spaces.Box(low=0, high=1, 
                                      shape=(self.num_operations + self.num_machines, 8), 
                                      dtype=np.float32),
            'adjacency': spaces.Box(low=0, high=1, 
                                  shape=(self.num_operations + self.num_machines, 
                                        self.num_operations + self.num_machines),
                                  dtype=np.float32),
            'global_features': spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)
        })
        
        self.reset()
    
    def reset(self, seed=None, options=None):
        if seed is not None:
            super().reset(seed=seed)
            
        # State tracking
        self.current_time = 0.0
        self.machine_available_time = {m: 0.0 for m in self.machines}
        self.operation_status = ['waiting'] * self.num_operations  # 'waiting', 'ready', 'running', 'done'
        self.operation_start_time = [None] * self.num_operations
        self.operation_end_time = [None] * self.num_operations
        self.job_next_op = {job_id: 0 for job_id in self.job_ids}
        self.schedule = {m: [] for m in self.machines}
        
        # Initialize arrived jobs (including jobs arriving at time 0)
        self.arrived_jobs = {
            job_id for job_id, arrival_time in self.job_arrival_times.items()
            if arrival_time <= self.current_time
        }
        
        # Update initial ready operations
        self._update_ready_operations()
        
        return self._get_observation(), {}
    
    def _update_ready_operations(self):
        """Update which operations are ready to be scheduled"""
        for job_id in self.job_ids:
            # Check if job has arrived
            if self.job_arrival_times[job_id] <= self.current_time:
                next_op_pos = self.job_next_op[job_id]
                if next_op_pos < len(self.jobs[job_id]):
                    op_idx = self.job_to_ops[job_id][next_op_pos]
                    
                    if self.operation_status[op_idx] == 'waiting':
                        # Check if previous operation is done
                        if next_op_pos == 0:  # First operation
                            self.operation_status[op_idx] = 'ready'
                        elif next_op_pos > 0:
                            prev_op_idx = self.job_to_ops[job_id][next_op_pos - 1]
                            if self.operation_status[prev_op_idx] == 'done':
                                self.operation_status[op_idx] = 'ready'
    
    def _get_valid_actions(self):
        """Get mask of valid actions"""
        mask = np.zeros(self.action_space.n, dtype=bool)
        
        # If no operations are ready, advance time to next arrival
        if not any(self.operation_status[i] == 'ready' for i in range(self.num_operations)):
            # Find next job arrival
            future_arrivals = [
                arrival for arrival in self.job_arrival_times.values() 
                if arrival > self.current_time
            ]
            
            if future_arrivals:
                next_time = min(future_arrivals)
                self.current_time = next_time
                
                # Update arrived jobs
                self.arrived_jobs.update({
                    job_id for job_id, arrival_time in self.job_arrival_times.items()
                    if arrival_time <= self.current_time
                })
                
                # Update ready operations
                self._update_ready_operations()
        
        # Mark valid actions
        for action_idx, (op_idx, machine) in enumerate(self.valid_action_pairs):
            job_id, op_pos = self.operations[op_idx]
            
            if (self.operation_status[op_idx] == 'ready' and 
                machine in self.jobs[job_id][op_pos]['proc_times']):
                mask[action_idx] = True
                
        return mask
    
    def action_masks(self):
        """Return action masks for MaskablePPO"""
        return self._get_valid_actions()
    
    def step(self, action):
        if action >= len(self.valid_action_pairs):
            return self._get_observation(), -1000, True, False, {"error": "Invalid action"}
        
        op_idx, machine = self.valid_action_pairs[action]
        job_id, op_pos = self.operations[op_idx]
        
        # Validate action
        if self.operation_status[op_idx] != 'ready':
            return self._get_observation(), -1000, True, False, {"error": "Operation not ready"}
        
        # Calculate timing
        job_ready_time = self.job_arrival_times[job_id]
        if op_pos > 0:
            prev_op_idx = self.job_to_ops[job_id][op_pos - 1]
            if self.operation_end_time[prev_op_idx] is not None:
                job_ready_time = max(job_ready_time, self.operation_end_time[prev_op_idx])
        
        machine_ready_time = self.machine_available_time[machine]
        start_time = max(job_ready_time, machine_ready_time, self.current_time)
        
        proc_time = self.jobs[job_id][op_pos]['proc_times'][machine]
        end_time = start_time + proc_time
        
        # Update state
        self.operation_status[op_idx] = 'done'
        self.operation_start_time[op_idx] = start_time
        self.operation_end_time[op_idx] = end_time
        self.machine_available_time[machine] = end_time
        self.job_next_op[job_id] += 1
        self.current_time = max(self.current_time, end_time)
        
        # Update schedule
        self.schedule[machine].append((f"J{job_id}-O{op_pos+1}", start_time, end_time))
        
        # Update ready operations
        self._update_ready_operations()
        
        # Calculate reward
        makespan = max(self.machine_available_time.values())
        reward = -proc_time - (start_time - machine_ready_time)  # Minimize processing time and idle time
        
        # Check if done
        done = all(self.operation_status[i] == 'done' for i in range(self.num_operations))
        if done:
            reward -= makespan * 0.1  # Final penalty based on makespan
        
        info = {"makespan": makespan}
        return self._get_observation(), reward, done, False, info
    
    def _get_observation(self):
        """Generate graph-based observation"""
        num_nodes = self.num_operations + self.num_machines
        
        # Node features (8 features per node)
        node_features = np.zeros((num_nodes, 8))
        
        # Operation nodes (first num_operations nodes)
        for i in range(self.num_operations):
            job_id, op_pos = self.operations[i]
            
            # Features: [is_ready, is_done, proc_time_normalized, position_in_job, 
            #           job_arrival_normalized, job_progress, urgency, flexibility]
            node_features[i, 0] = 1.0 if self.operation_status[i] == 'ready' else 0.0
            node_features[i, 1] = 1.0 if self.operation_status[i] == 'done' else 0.0
            
            # Average processing time (normalized)
            avg_proc_time = np.mean(list(self.jobs[job_id][op_pos]['proc_times'].values()))
            node_features[i, 2] = avg_proc_time / 10.0  # Normalize by max expected proc time
            
            node_features[i, 3] = op_pos / len(self.jobs[job_id])  # Position in job
            node_features[i, 4] = self.job_arrival_times[job_id] / 50.0  # Normalized arrival time
            node_features[i, 5] = self.job_next_op[job_id] / len(self.jobs[job_id])  # Job progress
            
            # Urgency (inverse of remaining processing time)
            remaining_ops = len(self.jobs[job_id]) - self.job_next_op[job_id]
            node_features[i, 6] = 1.0 / (remaining_ops + 1)
            
            # Flexibility (number of machine options)
            node_features[i, 7] = len(self.jobs[job_id][op_pos]['proc_times']) / self.num_machines
        
        # Machine nodes (last num_machines nodes)
        for i in range(self.num_machines):
            machine = self.machines[i]
            machine_idx = self.num_operations + i
            
            # Features: [availability_normalized, utilization, current_load, queue_length, 
            #           last_job_end, efficiency, workload_balance, flexibility]
            node_features[machine_idx, 0] = min(self.machine_available_time[machine] / 50.0, 1.0)
            
            # Calculate utilization and other metrics
            total_work = sum(end - start for _, start, end in self.schedule[machine])
            node_features[machine_idx, 1] = total_work / max(self.current_time, 1.0)
            
            # Current load (operations scheduled)
            node_features[machine_idx, 2] = len(self.schedule[machine]) / 20.0  # Normalize
            
            # Other features can be enhanced based on specific requirements
            node_features[machine_idx, 3] = 0.5  # Placeholder for queue length
            node_features[machine_idx, 4] = 0.5  # Placeholder
            node_features[machine_idx, 5] = 0.5  # Placeholder for efficiency
            node_features[machine_idx, 6] = 0.5  # Placeholder for workload balance
            node_features[machine_idx, 7] = 0.5  # Placeholder for flexibility
        
        # Adjacency matrix (simplified - can be enhanced)
        adjacency = np.zeros((num_nodes, num_nodes))
        
        # Job precedence relationships
        for job_id in self.job_ids:
            ops = self.job_to_ops[job_id]
            for i in range(len(ops) - 1):
                adjacency[ops[i], ops[i+1]] = 1.0  # Precedence
        
        # Operation-machine compatibility
        for op_idx, (job_id, op_pos) in enumerate(self.operations):
            for machine in self.jobs[job_id][op_pos]['proc_times']:
                machine_idx = self.machines.index(machine) + self.num_operations
                adjacency[op_idx, machine_idx] = 1.0  # Can process
                adjacency[machine_idx, op_idx] = 1.0  # Bidirectional
        
        # Global features
        global_features = np.array([
            self.current_time / 50.0,  # Normalized current time
            sum(1 for s in self.operation_status if s == 'done') / self.num_operations,  # Progress
            max(self.machine_available_time.values()) / 50.0,  # Normalized makespan
            len(self.arrived_jobs) / self.num_jobs,  # Job arrival progress
            sum(1 for s in self.operation_status if s == 'ready') / self.num_operations,  # Ready ops
            np.mean(list(self.machine_available_time.values())) / 50.0,  # Avg machine availability
            np.std(list(self.machine_available_time.values())) / 10.0,  # Machine balance
            0.5  # Placeholder for additional global feature
        ])
        
        return {
            'node_features': node_features.astype(np.float32),
            'adjacency': adjacency.astype(np.float32),
            'global_features': global_features.astype(np.float32)
        }

# --- 3. Graph Neural Network Feature Extractor ---
class GraphFeaturesExtractor(BaseFeaturesExtractor):
    """
    Enhanced Graph Neural Network feature extractor for DFJSP
    """
    
    def __init__(self, observation_space, features_dim=128):
        super().__init__(observation_space, features_dim)
        
        # Extract dimensions from observation space
        node_features_dim = observation_space['node_features'].shape[1]  # Should be 8
        global_features_dim = observation_space['global_features'].shape[0]  # Should be 8
        
        # Graph convolution layers
        self.node_embedding = nn.Linear(node_features_dim, 64)
        self.gnn1 = nn.Linear(64, 64)
        self.gnn2 = nn.Linear(64, 64)
        
        # Global feature processing
        self.global_processing = nn.Sequential(
            nn.Linear(global_features_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 32)
        )
        
        # Final feature combination
        self.final_layer = nn.Sequential(
            nn.Linear(64 + 32, features_dim),  # 64 from graph + 32 from global
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
    def forward(self, observations):
        device = next(self.parameters()).device
        
        # Move tensors to correct device and handle batch dimension
        if isinstance(observations, dict):
            batch_size = None
            for key in observations:
                obs_tensor = torch.FloatTensor(observations[key]).to(device)
                if batch_size is None:
                    batch_size = obs_tensor.shape[0] if len(obs_tensor.shape) > len(self.observation_space[key].shape) else 1
                    if batch_size == 1 and len(obs_tensor.shape) == len(self.observation_space[key].shape):
                        obs_tensor = obs_tensor.unsqueeze(0)
                observations[key] = obs_tensor
        
        node_features = observations['node_features']  # [batch, nodes, features]
        adjacency = observations['adjacency']         # [batch, nodes, nodes]  
        global_features = observations['global_features'] # [batch, global_features]
        
        batch_size = node_features.shape[0]
        
        # Initial node embedding
        node_embeddings = torch.relu(self.node_embedding(node_features))  # [batch, nodes, 64]
        
        # Graph convolution 1
        # For each node, aggregate information from neighbors
        messages = torch.bmm(adjacency, node_embeddings)  # [batch, nodes, 64]
        node_embeddings = torch.relu(self.gnn1(node_embeddings + messages))
        
        # Graph convolution 2  
        messages = torch.bmm(adjacency, node_embeddings)
        node_embeddings = torch.relu(self.gnn2(node_embeddings + messages))
        
        # Graph-level aggregation (mean pooling)
        graph_features = torch.mean(node_embeddings, dim=1)  # [batch, 64]
        
        # Process global features
        processed_global = self.global_processing(global_features)  # [batch, 32]
        
        # Combine graph and global features
        combined_features = torch.cat([graph_features, processed_global], dim=1)  # [batch, 96]
        
        # Final transformation
        output = self.final_layer(combined_features)  # [batch, features_dim]
        
        return output

class EnhancedGraphMaskableActorCriticPolicy(MaskableActorCriticPolicy):
    def __init__(self, *args, **kwargs):
        kwargs['features_extractor_class'] = GraphFeaturesExtractor
        kwargs['features_extractor_kwargs'] = {'features_dim': 128}
        super().__init__(*args, **kwargs)

# --- 4. Training and Evaluation ---
def train_agent(jobs_data, machine_list, train_arrivals, log_name, total_timesteps=150000):
    print(f"\n--- Training Agent: {log_name} ---")
    
    def make_train_env():
        env = GraphDFJSPEnv(jobs_data, machine_list, train_arrivals)
        return ActionMasker(env, mask_fn)

    vec_env = DummyVecEnv([make_train_env])

    # Enhanced hyperparameters for better GraphDFJSPEnv training
    model = MaskablePPO(
        EnhancedGraphMaskableActorCriticPolicy,
        vec_env,
        verbose=1,
        tensorboard_log=f"./fjsp_rl_logs/",
        device="auto",
        n_steps=2048,        # Larger rollout buffer
        batch_size=128,      # Reasonable batch size for stable gradients
        n_epochs=8,          # More epochs per update
        learning_rate=2e-4,  # Lower learning rate for more stable convergence
        ent_coef=0.02,       # Higher entropy for better exploration
        clip_range=0.3,      # Slightly higher clipping range
        clip_range_vf=None,  # No value function clipping
        vf_coef=0.8,         # Higher value function coefficient
        max_grad_norm=0.8,   # Higher gradient clipping threshold
        target_kl=0.01,      # Early stopping based on KL divergence
        gamma=0.99,          # Standard discount factor
        gae_lambda=0.95      # GAE parameter
    )
    
    # Save initial model
    model_save_path = f"./best_fjsp_model_{log_name}"
    os.makedirs(model_save_path, exist_ok=True)
    
    print(f"Training for {total_timesteps} timesteps...")
    model.learn(total_timesteps=total_timesteps, progress_bar=True)
    
    # Save trained model
    model.save(f"{model_save_path}/final_model")
    print(f"Model saved to {model_save_path}/final_model")
    
    return model

def evaluate_agent(model, jobs_data, machine_list, eval_arrivals, scenario_name):
    print(f"\n--- Evaluating Agent on Scenario: {scenario_name} ---")
    
    def make_eval_env():
        return GraphDFJSPEnv(jobs_data, machine_list, eval_arrivals)

    eval_env = make_eval_env()
    obs, _ = eval_env.reset()
    done = False
    
    while not done:
        action_masks = get_action_masks(eval_env)
        action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
        obs, _, done, _, info = eval_env.step(action)

    final_makespan = info.get("makespan", float('inf'))
    print(f"Evaluation complete. Final Makespan: {final_makespan:.2f}")
    
    return final_makespan, eval_env.schedule

# --- 5. Heuristic Scheduler for Comparison ---
def heuristic_spt_scheduler(jobs_data, machine_list, job_arrival_times):
    """
    Schedules jobs based on the Shortest Processing Time (SPT) heuristic,
    considering dynamic job arrivals.
    """
    print("\n--- Running SPT Heuristic Scheduler ---")
    
    machine_next_free = {m: 0.0 for m in machine_list}
    operation_end_times = {job_id: [0.0] * len(jobs_data[job_id]) for job_id in jobs_data}
    next_operation_for_job = {job_id: 0 for job_id in jobs_data}
    
    schedule = {m: [] for m in machine_list}
    operations_scheduled_count = 0
    total_operations = sum(len(ops) for ops in jobs_data.values())
    
    arrived_jobs = {job_id for job_id, arrival in job_arrival_times.items() if arrival <= 0}
    
    while operations_scheduled_count < total_operations:
        candidate_operations = []
        
        # Find the earliest time a machine becomes free to advance time
        if not any(next_operation_for_job[job_id] < len(jobs_data[job_id]) for job_id in arrived_jobs):
             # If no available jobs have pending operations, advance time to next arrival
            upcoming_arrivals = [arr for arr in job_arrival_times.values() if arr > min(machine_next_free.values())]
            if not upcoming_arrivals: 
                break # No more jobs will arrive
            
            next_arrival_time = min(upcoming_arrivals)
            for m in machine_list:
                if machine_next_free[m] < next_arrival_time:
                    machine_next_free[m] = next_arrival_time
            
            arrived_jobs.update({job_id for job_id, arrival in job_arrival_times.items() if arrival <= next_arrival_time})

        for job_id in arrived_jobs:
            op_idx = next_operation_for_job[job_id]
            if op_idx < len(jobs_data[job_id]):
                operation = jobs_data[job_id][op_idx]
                
                # Check if this operation can start (precedence constraint)
                can_start = True
                if op_idx > 0:
                    prev_end_time = operation_end_times[job_id][op_idx - 1]
                    if prev_end_time <= 0:
                        can_start = False
                
                if can_start:
                    # Find the machine with shortest processing time for this operation
                    proc_times = operation['proc_times']
                    best_machine = min(proc_times.keys(), key=lambda m: proc_times[m])
                    best_proc_time = proc_times[best_machine]
                    
                    # Calculate the earliest start time for this operation
                    machine_available = machine_next_free[best_machine]
                    job_ready_time = max(job_arrival_times[job_id], 
                                       operation_end_times[job_id][op_idx - 1] if op_idx > 0 else 0)
                    earliest_start = max(machine_available, job_ready_time)
                    
                    candidate_operations.append((
                        job_id, op_idx, best_machine, best_proc_time, earliest_start
                    ))
        
        if not candidate_operations:
            break
        
        # Select operation with shortest processing time (SPT heuristic)
        selected = min(candidate_operations, key=lambda x: x[3])  # x[3] is proc_time
        job_id, op_idx, machine, proc_time, start_time = selected
        
        end_time = start_time + proc_time
        
        # Update state
        machine_next_free[machine] = end_time
        operation_end_times[job_id][op_idx] = end_time
        next_operation_for_job[job_id] += 1
        operations_scheduled_count += 1
        
        # Add to schedule
        schedule[machine].append((f"J{job_id}-O{op_idx+1}", start_time, end_time))
    
    makespan = max(machine_next_free.values())
    print(f"SPT Heuristic Makespan: {makespan:.2f}")
    
    return makespan, schedule

def heuristic_fifo_scheduler(jobs_data, machine_list, job_arrival_times):
    """FIFO (First In First Out) scheduler - schedules jobs in arrival order"""
    print("Running FIFO scheduler...")
    
    # Sort jobs by arrival time, then by job ID
    sorted_jobs = sorted(job_arrival_times.items(), key=lambda x: (x[1], x[0]))
    
    machine_next_free = {m: 0.0 for m in machine_list}
    schedule = {m: [] for m in machine_list}
    
    for job_id, arrival_time in sorted_jobs:
        operations = jobs_data[job_id]
        job_completion_time = arrival_time
        
        for op_idx, op_data in enumerate(operations):
            # Choose first available machine that can process this operation
            available_machines = [m for m in machine_list if m in op_data['proc_times']]
            
            if available_machines:
                # Select machine that will be free earliest
                selected_machine = min(available_machines, 
                                     key=lambda m: machine_next_free[m])
                
                proc_time = op_data['proc_times'][selected_machine]
                start_time = max(machine_next_free[selected_machine], job_completion_time)
                end_time = start_time + proc_time
                
                schedule[selected_machine].append((f"J{job_id}-O{op_idx+1}", start_time, end_time))
                machine_next_free[selected_machine] = end_time
                job_completion_time = end_time
    
    makespan = max(machine_next_free.values())
    print(f"FIFO Heuristic Makespan: {makespan:.2f}")
    
    return makespan, schedule

def heuristic_random_scheduler(jobs_data, machine_list, job_arrival_times):
    """Random scheduler - makes random valid choices"""
    print("Running Random scheduler...")
    
    # Set seed for reproducible random baseline
    random.seed(123)
    
    machine_next_free = {m: 0.0 for m in machine_list}
    schedule = {m: [] for m in machine_list}
    
    # Create list of all operations to schedule
    operations_to_schedule = []
    for job_id in jobs_data:
        for op_idx in range(len(jobs_data[job_id])):
            operations_to_schedule.append((job_id, op_idx))
    
    # Shuffle operations randomly
    random.shuffle(operations_to_schedule)
    
    scheduled_ops = set()
    remaining_ops = operations_to_schedule.copy()
    
    while remaining_ops:
        # Find operations that can be scheduled (predecessors are done)
        schedulable_ops = []
        for job_id, op_idx in remaining_ops:
            # Check if previous operation is done
            if op_idx == 0 or (job_id, op_idx-1) in scheduled_ops:
                # Check if job has arrived
                if job_arrival_times[job_id] <= max(machine_next_free.values()):
                    schedulable_ops.append((job_id, op_idx))
        
        if not schedulable_ops:
            # Advance time to next job arrival
            future_arrivals = [t for t in job_arrival_times.values() 
                             if t > max(machine_next_free.values())]
            if future_arrivals:
                next_arrival = min(future_arrivals)
                # Update machine times
                min_time = max(machine_next_free.values())
                for m in machine_list:
                    machine_next_free[m] = max(machine_next_free[m], next_arrival)
            continue
        
        # Randomly select one schedulable operation
        job_id, op_idx = random.choice(schedulable_ops)
        op_data = jobs_data[job_id][op_idx]
        
        # Randomly select a machine that can process this operation
        available_machines = [m for m in machine_list if m in op_data['proc_times']]
        selected_machine = random.choice(available_machines)
        
        # Calculate timing
        job_ready_time = job_arrival_times[job_id]
        if op_idx > 0:
            # Find when previous operation completed
            prev_ops = [(m, ops) for m, ops in schedule.items() for ops in ops 
                       if ops[0] == f"J{job_id}-O{op_idx}"]
            if prev_ops:
                job_ready_time = max(job_ready_time, 
                                   max(end_time for _, _, end_time in prev_ops))
        
        start_time = max(machine_next_free[selected_machine], job_ready_time)
        proc_time = op_data['proc_times'][selected_machine]
        end_time = start_time + proc_time
        
        schedule[selected_machine].append((f"J{job_id}-O{op_idx+1}", start_time, end_time))
        machine_next_free[selected_machine] = end_time
        
        scheduled_ops.add((job_id, op_idx))
        remaining_ops.remove((job_id, op_idx))
    
    makespan = max(machine_next_free.values())
    print(f"Random Heuristic Makespan: {makespan:.2f}")
    
    return makespan, schedule

def milp_optimal_scheduler(jobs_data, machine_list, job_arrival_times, time_limit=300):
    """
    MILP-based optimal scheduler for Dynamic FJSP using PuLP.
    
    Args:
        jobs_data: Dictionary of jobs with operations and processing times
        machine_list: List of machine names
        job_arrival_times: Dictionary of job arrival times
        time_limit: Time limit for solver in seconds
    
    Returns:
        makespan, schedule: Optimal makespan and schedule dictionary
    """
    print("\n--- Running MILP Optimal Scheduler ---")
    print(f"Solver time limit: {time_limit} seconds")
    
    # Create problem
    prob = LpProblem("Dynamic_FJSP_Makespan_Minimization", LpMinimize)
    
    # Extract problem parameters
    jobs = list(jobs_data.keys())
    machines = machine_list
    
    # Calculate large M (big-M constraint parameter)
    total_processing_time = sum(
        max(op['proc_times'].values()) 
        for job_ops in jobs_data.values() 
        for op in job_ops
    )
    max_arrival = max(job_arrival_times.values()) if job_arrival_times else 0
    M = total_processing_time + max_arrival + 1000
    
    # Decision variables
    # x[j][op][m] = 1 if job j operation op is assigned to machine m
    x = {}
    for j in jobs:
        x[j] = {}
        for op in range(len(jobs_data[j])):
            x[j][op] = {}
            for m in machines:
                if m in jobs_data[j][op]['proc_times']:
                    x[j][op][m] = LpVariable(f"x_{j}_{op}_{m}", cat='Binary')
                else:
                    x[j][op][m] = 0  # Not feasible assignment
    
    # Start times s[j][op]
    s = {}
    for j in jobs:
        s[j] = {}
        for op in range(len(jobs_data[j])):
            s[j][op] = LpVariable(f"s_{j}_{op}", lowBound=0, cat='Continuous')
    
    # Makespan variable
    C_max = LpVariable("makespan", lowBound=0, cat='Continuous')
    
    # Binary variables for operation ordering on machines: y[j1][op1][j2][op2][m]
    # y = 1 if (j1,op1) precedes (j2,op2) on machine m
    y = {}
    for j1 in jobs:
        for op1 in range(len(jobs_data[j1])):
            for j2 in jobs:
                for op2 in range(len(jobs_data[j2])):
                    if (j1, op1) != (j2, op2):  # Different operations
                        for m in machines:
                            if (m in jobs_data[j1][op1]['proc_times'] and 
                                m in jobs_data[j2][op2]['proc_times']):
                                y[(j1, op1, j2, op2, m)] = LpVariable(
                                    f"y_{j1}_{op1}_{j2}_{op2}_{m}", cat='Binary'
                                )
    
    # Objective: Minimize makespan
    prob += C_max
    
    # Constraints
    
    # 1. Each operation must be assigned to exactly one machine
    for j in jobs:
        for op in range(len(jobs_data[j])):
            prob += lpSum([x[j][op][m] for m in machines 
                          if m in jobs_data[j][op]['proc_times']]) == 1
    
    # 2. Precedence constraints within jobs
    for j in jobs:
        for op in range(1, len(jobs_data[j])):
            # Current operation starts after previous operation finishes
            prev_completion = (s[j][op-1] + 
                             lpSum([x[j][op-1][m] * jobs_data[j][op-1]['proc_times'][m]
                                   for m in machines if m in jobs_data[j][op-1]['proc_times']]))
            prob += s[j][op] >= prev_completion
    
    # 3. Job arrival time constraints
    for j in jobs:
        prob += s[j][0] >= job_arrival_times[j]
    
    # 4. Machine capacity constraints (no overlapping operations on same machine)
    for j1 in jobs:
        for op1 in range(len(jobs_data[j1])):
            for j2 in jobs:
                for op2 in range(len(jobs_data[j2])):
                    if (j1, op1) != (j2, op2):
                        for m in machines:
                            if (m in jobs_data[j1][op1]['proc_times'] and 
                                m in jobs_data[j2][op2]['proc_times']):
                                
                                # If both operations assigned to machine m, enforce ordering
                                prob += (s[j2][op2] >= 
                                        s[j1][op1] + jobs_data[j1][op1]['proc_times'][m] - 
                                        M * (3 - x[j1][op1][m] - x[j2][op2][m] - y[(j1, op1, j2, op2, m)]))
                                
                                prob += (s[j1][op1] >= 
                                        s[j2][op2] + jobs_data[j2][op2]['proc_times'][m] - 
                                        M * (2 - x[j1][op1][m] - x[j2][op2][m] + y[(j1, op1, j2, op2, m)]))
    
    # 5. Makespan constraints
    for j in jobs:
        last_op = len(jobs_data[j]) - 1
        completion_time = (s[j][last_op] + 
                          lpSum([x[j][last_op][m] * jobs_data[j][last_op]['proc_times'][m]
                                for m in machines if m in jobs_data[j][last_op]['proc_times']]))
        prob += C_max >= completion_time
    
    # Solve the problem
    print("Starting MILP optimization...")
    solver = PULP_CBC_CMD(timeLimit=time_limit, msg=1)
    prob.solve(solver)
    
    # Check solution status
    if prob.status != 1:  # Not optimal
        print(f"MILP solver status: {prob.status}")
        if prob.status == -1:
            print("MILP solver could not find optimal solution within time limit")
            print("Returning best feasible solution found...")
        else:
            print("MILP solver failed - returning None")
            return None, None
    
    # Extract solution
    makespan = C_max.value()
    print(f"MILP Optimal Makespan: {makespan:.2f}")
    
    # Build schedule
    schedule = {m: [] for m in machines}
    
    for j in jobs:
        for op in range(len(jobs_data[j])):
            for m in machines:
                if (m in jobs_data[j][op]['proc_times'] and 
                    isinstance(x[j][op][m], LpVariable) and 
                    x[j][op][m].value() > 0.5):
                    
                    start_time = s[j][op].value()
                    proc_time = jobs_data[j][op]['proc_times'][m]
                    end_time = start_time + proc_time
                    
                    schedule[m].append((f"J{j}-O{op+1}", start_time, end_time))
    
    # Sort operations by start time for each machine
    for m in machines:
        schedule[m].sort(key=lambda x: x[1])
    
    return makespan, schedule

def plot_four_method_comparison(methods_data, machines, title, save_path):
    """Create a 2x2 subplot comparison of four scheduling methods"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(title, fontsize=16, fontweight='bold')
    
    colors = plt.cm.tab20.colors
    methods = list(methods_data.keys())
    
    for idx, (method_name, (schedule, makespan)) in enumerate(methods_data.items()):
        row = idx // 2
        col = idx % 2
        ax = axes[row, col]
        
        # Plot Gantt chart for this method
        if schedule and any(len(ops) > 0 for ops in schedule.values()):
            for machine_idx, machine in enumerate(machines):
                machine_ops = schedule.get(machine, [])
                machine_ops.sort(key=lambda x: x[1])
                
                for op_data in machine_ops:
                    if len(op_data) == 3:
                        job_id_str, start, end = op_data
                        try:
                            j = int(job_id_str.split('-')[0][1:])
                        except (ValueError, IndexError):
                            j = hash(job_id_str) % len(colors)
                        
                        ax.broken_barh(
                            [(start, end - start)],
                            (machine_idx * 10, 8),
                            facecolors=colors[j % len(colors)],
                            edgecolor='black',
                            alpha=0.8
                        )
                        ax.text(start + (end - start) / 2, machine_idx * 10 + 4,
                               job_id_str, color='white', fontsize=8,
                               ha='center', va='center', weight='bold')
        
        ax.set_yticks([i * 10 + 4 for i in range(len(machines))])
        ax.set_yticklabels(machines)
        ax.set_xlabel("Time")
        ax.set_ylabel("Machines")
        ax.set_title(f"{method_name}\nMakespan: {makespan:.1f}")
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"Saved four-method comparison to {save_path}")

def train_static_rl_agent(jobs_data, machine_list, total_timesteps=50000):
    """Train a static RL agent (no dynamic arrivals)"""
    print("Training Static RL agent (no dynamic arrivals)...")
    
    # Create static version of jobs data (all arrive at time 0)
    static_arrivals = {job_id: 0 for job_id in jobs_data.keys()}
    
    def make_env():
        env = GraphDFJSPEnv(jobs_data, machine_list, static_arrivals)
        return ActionMasker(env, mask_fn)
    
    vec_env = DummyVecEnv([make_env])
    
    model = MaskablePPO(
        MaskableActorCriticPolicy,
        vec_env,
        verbose=1,
        tensorboard_log="./static_rl_logs/",
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01
    )
    
    model.learn(total_timesteps=total_timesteps, progress_bar=True)
    return model

def evaluate_static_rl_agent(model, jobs_data, machine_list, job_arrival_times):
    """Evaluate static RL agent on dynamic problem"""
    print("Evaluating Static RL agent on dynamic arrival problem...")
    
    # Test the static RL agent on the dynamic problem
    env = GraphDFJSPEnv(jobs_data, machine_list, job_arrival_times)
    env = ActionMasker(env, mask_fn)
    
    obs, _ = env.reset()
    total_reward = 0
    step_count = 0
    
    while step_count < 100:  # Safety limit
        action_masks = get_action_masks(env)
        if not action_masks.any():
            break
            
        action, _ = model.predict(obs, action_masks=action_masks, deterministic=True)
        obs, reward, done, truncated, info = env.step(action)
        total_reward += reward
        step_count += 1
        
        if done or truncated:
            break
    
    makespan = info.get('makespan', env.current_makespan)
    print(f"Static RL Makespan on Dynamic Problem: {makespan:.2f}")
    
    return makespan, env.schedule

def plot_static_vs_dynamic_rl_comparison(methods_data, machines, title, save_path):
    """Create side-by-side comparison of Static vs Dynamic RL"""
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    fig.suptitle(title, fontsize=16, fontweight='bold')
    
    colors = plt.cm.tab20.colors
    methods = list(methods_data.keys())
    
    for idx, (method_name, (schedule, makespan)) in enumerate(methods_data.items()):
        ax = axes[idx]
        
        # Plot Gantt chart
        if schedule and any(len(ops) > 0 for ops in schedule.values()):
            for machine_idx, machine in enumerate(machines):
                machine_ops = schedule.get(machine, [])
                machine_ops.sort(key=lambda x: x[1])
                
                for op_data in machine_ops:
                    if len(op_data) == 3:
                        job_id_str, start, end = op_data
                        try:
                            j = int(job_id_str.split('-')[0][1:])
                        except (ValueError, IndexError):
                            j = hash(job_id_str) % len(colors)
                        
                        ax.broken_barh(
                            [(start, end - start)],
                            (machine_idx * 10, 8),
                            facecolors=colors[j % len(colors)],
                            edgecolor='black',
                            alpha=0.8
                        )
                        ax.text(start + (end - start) / 2, machine_idx * 10 + 4,
                               job_id_str, color='white', fontsize=9,
                               ha='center', va='center', weight='bold')
        
        ax.set_yticks([i * 10 + 4 for i in range(len(machines))])
        ax.set_yticklabels(machines)
        ax.set_xlabel("Time")
        ax.set_ylabel("Machines")
        ax.set_title(f"{method_name}\nMakespan: {makespan:.1f}")
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"Saved Static vs Dynamic RL comparison to {save_path}")

if __name__ == "__main__":
    """
    Main execution demonstrating DRL training and evaluation for Dynamic FJSP
    """
    print("="*80)
    print("DYNAMIC FJSP - DEEP REINFORCEMENT LEARNING PIPELINE")
    print("="*80)
    
    # Set random seed for reproducibility
    random.seed(42)
    np.random.seed(42)
    
    print("\n1. TRAINING DRL AGENT")
    print("-" * 50)
    
    # Train on default instance
    print("Training on default 4-job instance...")
    trained_model = train_agent(
        DEFAULT_JOBS_DATA, 
        DEFAULT_MACHINE_LIST, 
        DEFAULT_ARRIVAL_TIMES, 
        "default_instance",
        total_timesteps=100000
    )
    
    print("\n2. EVALUATING DRL AGENT")
    print("-" * 50)
    
    # Evaluate on default instance
    print("Evaluating on training instance...")
    drl_makespan_default, drl_schedule_default = evaluate_agent(
        trained_model, 
        DEFAULT_JOBS_DATA, 
        DEFAULT_MACHINE_LIST, 
        DEFAULT_ARRIVAL_TIMES,
        "Default Training Instance"
    )
    
    # Evaluate on extended instance
    print("Evaluating on extended 7-job instance...")
    drl_makespan_extended, drl_schedule_extended = evaluate_agent(
        trained_model, 
        EXTENDED_JOBS_DATA, 
        EXTENDED_MACHINE_LIST, 
        EXTENDED_ARRIVAL_TIMES,
        "Extended 7-Job Instance"
    )
    
    print("\n3. HEURISTIC BASELINE COMPARISON")
    print("-" * 50)
    
    # Compare with SPT heuristic
    spt_makespan_default, spt_schedule_default = heuristic_spt_scheduler(
        DEFAULT_JOBS_DATA, DEFAULT_MACHINE_LIST, DEFAULT_ARRIVAL_TIMES
    )
    
    spt_makespan_extended, spt_schedule_extended = heuristic_spt_scheduler(
        EXTENDED_JOBS_DATA, EXTENDED_MACHINE_LIST, EXTENDED_ARRIVAL_TIMES
    )
    
    print("\n4. RESULTS SUMMARY")
    print("-" * 50)
    print("Default Instance (4 jobs):")
    print(f"  DRL Agent Makespan:     {drl_makespan_default:.2f}")
    print(f"  SPT Heuristic Makespan: {spt_makespan_default:.2f}")
    improvement_default = ((spt_makespan_default - drl_makespan_default) / spt_makespan_default * 100) if spt_makespan_default > 0 else 0
    print(f"  DRL Improvement:        {improvement_default:+.1f}%")
    
    print("\nExtended Instance (7 jobs):")
    print(f"  DRL Agent Makespan:     {drl_makespan_extended:.2f}")
    print(f"  SPT Heuristic Makespan: {spt_makespan_extended:.2f}")
    improvement_extended = ((spt_makespan_extended - drl_makespan_extended) / spt_makespan_extended * 100) if spt_makespan_extended > 0 else 0
    print(f"  DRL Improvement:        {improvement_extended:+.1f}%")
    
    print("\n5. GANTT CHART VISUALIZATION")
    print("-" * 50)
    
    # Create Gantt charts
    print("Generating comparison Gantt charts...")
    
    # Generate additional baseline solutions for comprehensive comparison
    print("Computing additional baselines...")
    
    # FIFO scheduler (First In First Out - simple baseline)
    fifo_makespan_default, fifo_schedule_default = heuristic_fifo_scheduler(
        DEFAULT_JOBS_DATA, DEFAULT_MACHINE_LIST, DEFAULT_ARRIVAL_TIMES
    )
    
    # Random scheduler (Monte Carlo baseline)
    random_makespan_default, random_schedule_default = heuristic_random_scheduler(
        DEFAULT_JOBS_DATA, DEFAULT_MACHINE_LIST, DEFAULT_ARRIVAL_TIMES
    )
    
    # Figure 1: Comparison of Four Methods on Default Instance
    plot_four_method_comparison(
        {
            'DRL Agent': (drl_schedule_default, drl_makespan_default),
            'SPT Heuristic': (spt_schedule_default, spt_makespan_default),
            'FIFO': (fifo_schedule_default, fifo_makespan_default),
            'Random': (random_schedule_default, random_makespan_default)
        },
        DEFAULT_MACHINE_LIST,
        "Comparison of Four Scheduling Methods - Default Instance",
        "figure1_main_comparison.png"
    )
    
    # Figure 2: Static RL vs Dynamic RL Comparison
    print("Training Static RL baseline...")
    static_rl_model = train_static_rl_agent(
        DEFAULT_JOBS_DATA, DEFAULT_MACHINE_LIST,
        total_timesteps=50000
    )
    
    print("Evaluating Static RL...")
    static_rl_makespan, static_rl_schedule = evaluate_static_rl_agent(
        static_rl_model, DEFAULT_JOBS_DATA, DEFAULT_MACHINE_LIST, DEFAULT_ARRIVAL_TIMES
    )
    
    plot_static_vs_dynamic_rl_comparison(
        {
            'Dynamic RL': (drl_schedule_default, drl_makespan_default),
            'Static RL': (static_rl_schedule, static_rl_makespan)
        },
        DEFAULT_MACHINE_LIST,
        "Static RL vs Dynamic RL Generalization",
        "figure2_static_rl_generalization.png"
    )
    
    print("\n6. PERFORMANCE ANALYSIS")
    print("-" * 50)
    
    if improvement_default > 0:
        print(f" DRL agent outperformed SPT heuristic on default instance by {improvement_default:.1f}%")
    else:
        print(f" DRL agent performance on default instance: {improvement_default:+.1f}%")
        
    if improvement_extended > 0:
        print(f" DRL agent outperformed SPT heuristic on extended instance by {improvement_extended:.1f}%")
    else:
        print(f" DRL agent performance on extended instance: {improvement_extended:+.1f}%")
    
    avg_improvement = (improvement_default + improvement_extended) / 2
    print(f"\n Average DRL improvement: {avg_improvement:+.1f}%")
    
    print("\n" + "="*80)
    print("DRL PIPELINE DEMONSTRATION COMPLETED")
    print("Generated comparison Gantt charts:")
    print("- figure1_main_comparison.png (Four Methods Comparison)")
    print("- figure2_static_rl_generalization.png (Static RL vs Dynamic RL)")
    print("="*80)
