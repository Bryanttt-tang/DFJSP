# üîç Analysis: Why Heuristics Outperform Perfect RL

## Date: October 31, 2025

---

## üìä Observations from Training

### 1. Perfect RL Performance Issues
- **Training:** Mean episode reward converges to ~-80
- **Final Entropy:** -1.5 (VERY HIGH!)
- **Policy State:** Still stochastic, not converged to deterministic
- **Makespan:** Still far from MILP optimal

### 2. Best Heuristic Performance
- **Makespan:** 76.65 (much better than current Perfect RL)
- **Strategy:** SPT machine selection + job sequencing rule
- **Execution:** Deterministic, fast, near-optimal

---

## üéØ ROOT CAUSE: Why is Entropy Still High?

### Problem: Policy Not Converging

**Entropy = -1.5 means:**
- Policy is outputting nearly **uniform probability** across valid actions
- Agent is essentially making **random choices** among valid options
- **NOT learning** a deterministic optimal policy

**For reference:**
- Entropy ‚âà 0: Deterministic policy (good!)
- Entropy ‚âà -2 to -3: Still very stochastic (bad!)
- Entropy ‚âà -1.5: Almost random! (very bad!)

### Why is Policy Not Converging?

#### Issue 1: **ent_coef = 0.01 is TOO HIGH**

Current setting:
```python
ent_coef=0.01  # Still encourages randomness!
```

**Problem:** PPO actively encourages exploration by maximizing entropy:
```python
Policy Loss = -Advantage * log(œÄ(a|s)) - ent_coef * H(œÄ)
                                          ‚Üë
                                  Encourages randomness!
```

With `ent_coef=0.01`, the algorithm is **fighting convergence** by actively maintaining randomness.

**For Perfect Knowledge (deterministic problem), we want:**
```python
ent_coef=0.0  # NO entropy bonus!
```

Or even use entropy **penalty** (negative ent_coef) to actively push toward deterministic policy!

---

#### Issue 2: **Learning Rate Too High**

Current setting:
```python
learning_rate=5e-4  # 0.0005
```

**Problem:** With sparse rewards and high action space, large learning rates cause:
- Overshooting optimal policies
- Oscillation instead of convergence
- Policy instability

**Recommendation:**
```python
learning_rate=1e-5  # 0.00001 (10x smaller!)
```

---

#### Issue 3: **Policy Network Architecture May Be Too Complex**

Current setting:
```python
net_arch=dict(
    pi=[512, 512, 256, 128],  # 4-layer policy network
    vf=[512, 256, 128]        # 3-layer value network
)
```

**Problem:** Overly complex network for a deterministic scheduling problem:
- More parameters = harder to converge
- Susceptible to overfitting to random noise
- Can memorize bad exploration patterns

**Recommendation:** Simpler network might converge better:
```python
net_arch=dict(
    pi=[256, 256],      # Simpler policy
    vf=[256, 128]       # Simpler value
)
```

---

## üèÜ Why Heuristics Work So Well

### The Heuristic Strategy (from code analysis):

```python
# Machine Selection: ALWAYS use SPT (Shortest Processing Time)
best_machine = min(op_data['proc_times'].keys(), 
                   key=lambda m: op_data['proc_times'][m])

# Job Sequencing: Use dispatching rule (FIFO/SPT/LPT/etc.)
if rule == "SPT":
    selected_op = min(ready_operations, 
                     key=lambda x: (x['proc_time'], x['arrival_time'], x['job_id']))
```

###Why This is Effective:

**1. SPT Machine Selection (Critical!)**
- Always assigns operation to **fastest available machine**
- Minimizes processing time for each operation
- Proven to be near-optimal for makespan minimization
- **Deterministic and consistent**

**2. Job Sequencing Heuristics**
- **FIFO:** Fair, simple, avoids starvation
- **SPT:** Minimizes total completion time
- **LPT:** Load balancing for long jobs
- All are **deterministic** rules

**3. Greedy but Effective**
- Makes locally optimal decisions
- No exploration/exploitation tradeoff
- No stochasticity
- Fast and consistent

**4. Problem-Specific Knowledge**
- Uses domain knowledge (SPT is good for makespan)
- Exploits problem structure (FJSP flexibility)
- No need to learn from scratch

---

## üìà Comparison: RL vs Heuristics

| Aspect | Perfect RL (Current) | Best Heuristic |
|--------|---------------------|----------------|
| **Policy** | Stochastic (entropy -1.5) | Deterministic |
| **Machine Selection** | Random (high entropy) | SPT (always fastest) |
| **Job Sequencing** | Random (high entropy) | Rule-based (SPT/FIFO) |
| **Learning Required** | Yes (100k+ timesteps) | No (instant) |
| **Consistency** | Variable (stochastic) | Always same |
| **Performance** | Poor (not converged) | Good (76.65) |
| **Domain Knowledge** | None (learned from scratch) | Embedded (SPT rule) |

---

## ‚úÖ SOLUTIONS

### Solution 1: Fix Entropy Issue (CRITICAL!)

```python
# BEFORE (WRONG):
ent_coef=0.01,  # Encourages randomness

# AFTER (CORRECT):
ent_coef=0.0,   # No entropy bonus
# OR
ent_coef=-0.01, # Entropy PENALTY (actively discourages randomness)
```

**Why this works:**
- Removes incentive for stochastic policy
- Allows policy to converge to deterministic
- Aligns with deterministic optimal solution

---

### Solution 2: Reduce Learning Rate

```python
# BEFORE:
learning_rate=5e-4,  # Too large

# AFTER:
learning_rate=1e-5,  # Much smaller, stable convergence
```

**Why this works:**
- Smaller steps toward optimum
- Less overshooting
- Smoother convergence

---

### Solution 3: Simplify Network

```python
# BEFORE:
net_arch=dict(
    pi=[512, 512, 256, 128],
    vf=[512, 256, 128]
)

# AFTER:
net_arch=dict(
    pi=[256, 256],    # Simpler
    vf=[256, 128]      # Simpler
)
```

**Why this works:**
- Fewer parameters = easier to converge
- Less prone to overfitting
- Faster training

---

### Solution 4: Imitation Learning / Behavior Cloning (RECOMMENDED!)

**Idea:** Use the heuristic to pre-train / guide the RL agent!

#### Approach A: Behavior Cloning Pre-training
```python
# 1. Collect expert demonstrations from best heuristic
expert_trajectories = collect_heuristic_demonstrations(
    heuristic="SPT", num_episodes=1000
)

# 2. Pre-train policy to imitate expert
model = MaskablePPO(...)
model.learn_from_demonstrations(expert_trajectories, epochs=50)

# 3. Fine-tune with RL
model.learn(total_timesteps=100000)
```

**Benefits:**
- Starts from good policy (near heuristic performance)
- RL only needs to improve upon heuristic
- Much faster convergence
- Higher final performance

#### Approach B: Reward Shaping with Heuristic Guidance
```python
def _calculate_reward_with_heuristic_guidance(self, action, ...):
    # Base reward (makespan increment)
    reward = -(current_makespan - previous_makespan) * 10.0
    
    # Bonus for matching heuristic action
    heuristic_action = self._get_spt_action()  # SPT machine selection
    if action == heuristic_action:
        reward += 5.0  # Encourage SPT-like behavior
    
    return reward
```

**Benefits:**
- Guides agent toward good behaviors
- Leverages domain knowledge
- Still allows improvement beyond heuristic

---

### Solution 5: Inject SPT Rule into Action Selection (Hybrid Approach)

**Idea:** Hard-code machine selection to SPT, only learn job sequencing!

```python
def action_masks_with_spt(self):
    """
    Modified action masking: for each job, only allow SPT machine.
    This reduces action space and injects domain knowledge.
    """
    mask = np.zeros(self.action_space.n, dtype=bool)
    
    for job_id in self.job_ids:
        if self.next_operation[job_id] < len(self.jobs[job_id]):
            op = self.jobs[job_id][self.next_operation[job_id]]
            
            # Find SPT machine (shortest processing time)
            spt_machine = min(op['proc_times'].keys(), 
                            key=lambda m: op['proc_times'][m])
            
            # Only allow SPT machine for this operation
            machine_idx = self.machines.index(spt_machine)
            action = job_id * len(self.machines) + machine_idx
            mask[action] = True
    
    return mask
```

**Benefits:**
- Drastically reduces action space
- Injects proven heuristic (SPT)
- Agent only learns job sequencing (easier problem!)
- Should match or beat heuristic performance

---

## üéØ RECOMMENDED ACTION PLAN

### Priority 1: Fix Entropy (IMMEDIATE)
```python
ent_coef=0.0,  # Remove randomness incentive
```

### Priority 2: Lower Learning Rate (IMMEDIATE)
```python
learning_rate=1e-5,  # Slower, stabler convergence
```

### Priority 3: Train Longer (IMMEDIATE)
```python
total_timesteps=500000,  # 5x more training
```

### Priority 4: Simplify Network (MEDIUM TERM)
```python
net_arch=dict(pi=[256, 256], vf=[256, 128])
```

### Priority 5: Hybrid SPT Approach (MEDIUM TERM)
- Inject SPT into action masking
- Reduce action space, learn only job sequencing

### Priority 6: Imitation Learning (LONG TERM)
- Pre-train on heuristic demonstrations
- Fine-tune with RL
- Best of both worlds

---

## üìä Expected Performance After Fixes

| Configuration | Expected Entropy | Expected Makespan | Notes |
|---------------|-----------------|-------------------|-------|
| **Current (broken)** | -1.5 | ~80-90 | Not converged |
| **Fix ent_coef=0** | -3 to -5 | ~70-75 | More deterministic |
| **+ Lower LR** | -5 to -8 | ~65-70 | Smoother convergence |
| **+ Longer training** | -8 to -12 | ~60-65 | Near convergence |
| **+ Hybrid SPT** | -10 to -15 | ~50-55 | Using domain knowledge |
| **+ Imitation** | -15+ | ~45-50 | Best performance |
| **Best Heuristic** | N/A (deterministic) | 76.65 | Current benchmark |
| **MILP Optimal** | N/A (deterministic) | ~73 | Theoretical optimum |

---

## üî¨ Diagnostic: Is Entropy the Problem?

### Test 1: Check Current Entropy
```python
# From training logs:
Final entropy: -1.5  ‚ùå VERY HIGH!

# Should be:
Target entropy: < -10 ‚úÖ (approaching deterministic)
```

### Test 2: Check Policy Determinism
```python
# Sample actions from same state multiple times
state = env.reset()
actions = [model.predict(state, deterministic=False)[0] for _ in range(10)]

# Count unique actions
unique_ratio = len(set(actions)) / len(actions)

# Current (broken): unique_ratio ‚âà 0.8-1.0 (random!)
# Target (good):     unique_ratio ‚âà 0.0-0.2 (deterministic!)
```

### Test 3: Compare Greedy vs Stochastic Performance
```python
# Greedy (deterministic=True)
greedy_makespan = evaluate(model, env, deterministic=True)

# Stochastic (deterministic=False)
stochastic_makespan = evaluate(model, env, deterministic=False)

# If greedy >> stochastic: Policy is too random!
# Current: greedy ‚âà stochastic (both bad!) ‚ùå
# Target:  greedy < stochastic (greedy is better) ‚úÖ
```

---

## ‚úÖ CONCLUSION

### Root Cause: **ent_coef=0.01 is preventing policy convergence!**

With entropy bonus, PPO is **actively fighting** convergence to deterministic policy, keeping entropy high at -1.5.

### Why Heuristics Win: **Deterministic + Domain Knowledge**

Heuristics use:
1. **SPT for machine selection** (proven near-optimal)
2. **Deterministic rules** (no randomness)
3. **Domain knowledge** (embedded expertise)

### The Fix: **Remove entropy bonus + lower learning rate + train longer**

```python
# Critical changes:
ent_coef=0.0,           # Remove randomness incentive  
learning_rate=1e-5,     # Slower, more stable
total_timesteps=500000, # Train longer

# Expected result:
Final entropy: < -10 (deterministic)
Makespan: 50-60 (near heuristic)
```

### Optional Enhancement: **Hybrid SPT + RL**

Inject SPT rule into action masking ‚Üí agent learns job sequencing only ‚Üí should match/beat heuristic!

**The path forward is clear: Fix entropy, learn from heuristics!** üöÄ
